{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple image inputs\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4-vision-preview\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"What are in these images? Is there any difference between them?\",\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "          },\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    ")\n",
    "print(response.choices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI key is valid\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from os import environ\n",
    "\n",
    "load_dotenv(\"../../env\")\n",
    "\n",
    "# token_id = environ[\"modal-token-id\"]\n",
    "# token_secret = environ[\"modal-token-secret\"]\n",
    "openai_api_key = environ[\"OPENAI_API_KEY\"]\n",
    "assert sum(openai_api_key.encode('ascii')) == 4241, \"OpenAI key is invalid\"\n",
    "print(\"OpenAI key is valid\")\n",
    "\n",
    "huggingface_api_key = environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "\n",
    "# assert sum(token_id.encode('ascii')) == 2299, \"Modal token ID is invalid\"\n",
    "# assert sum(token_secret.encode('ascii')) == 2075, \"Modal token secret is invalid\"\n",
    "# print(\"Modal keys retrieved from Google drive are valid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to upload a local image\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"data/Enjoying-convertible-car.jpg\"\n",
    "Image(image_path)#, width=200, height=100)\n",
    "\n",
    "# Getting the base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "\n",
    "headers = {\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"Authorization\": f\"Bearer {openai_api_key}\"\n",
    "}\n",
    "\n",
    "prompt = \"Whatâ€™s in this image?\"\n",
    "\n",
    "def prompt_image(prompt, image_path):\n",
    "  base64_image = encode_image(image_path)\n",
    "  \n",
    "  payload = {\n",
    "    \"model\": \"gpt-4-vision-preview\",\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": prompt\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "            }\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    \"max_tokens\": 300\n",
    "  }\n",
    "\n",
    "  response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "  return response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows two individuals in a blue convertible car. The person in the passenger seat is holding up a scarf or a piece of light fabric that is fluttering in the wind. Both individuals appear to be wearing sunglasses and are likely enjoying a sunny day, possibly out on a leisure drive. The environment around them includes trees which might be olive trees, and a small building structure in the background, suggesting a rural or Mediterranean setting. The scenery indicates a hilly or mountainous region, as you can see a landscape extending into the distance under a clear blue sky.\n"
     ]
    }
   ],
   "source": [
    "print(prompt_image(prompt, image_path).json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<People><wear><sunglasses><because of bright sunlight><in a sunny day>\n",
      "<Vehicle><move><on a road><to transport individuals><in a rural area>\n",
      "<Individual><wave><scarf><to express joy or freedom><while riding in a vehicle>\n",
      "<Person><operate><vehicle><to travel from one place to another><on a clear day>\n",
      "<Passenger><sit><in a vehicle><to accompany the driver or to reach a destination><during a journey>\n",
      "<Clothing><flutter><in the wind><due to motion of the vehicle and outside air currents><while being held aloft>\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"\"\" \n",
    "list all possible beliefs we can extract from this image, and express them in this format\n",
    "<this thing or person> <action> <another thing><for this reason or purpose><in these conditions >... the last 2 fields are not mandatory\n",
    "please keep the propositions with the <action> to leave <another thing> as an object/person name\n",
    "I want general answers, not specific to this image.\n",
    "For instance, for this image, I would expect something like:\n",
    "<people><drive><convertible><because there is no risk of rain><in a sunny day>\n",
    "not <the person in the driver seat><drive><the convertible><because he is happy><in a sunny day>\n",
    "For <thing>, <person> and <action> be as generic and short as possible.\n",
    "\"\"\"\n",
    "ans1 = prompt_image(prompt1, image_path)\n",
    "print(ans1.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = \"\"\" \n",
    "list all possible beliefs we can extract from this image, and express them as a python dictionary with the following format:\n",
    "{\n",
    "    \"condition\": <conditions observed from the picture such as a sunny day, ie the conditions leading to the rest of the beliefs, such as the objective and action>,\n",
    "    \"objective\": <the objective of the person or thing in the picture, after observing the conditions in the picture>,\n",
    "    \"subject\": <the person or thing in the picture doing the 'action' to meet the 'objective', just one word if possible>,\n",
    "    \"action\": <the action the person or thing in the picture is doing to meet the 'objective', expressed in one word with an optional preposition, such as 'drive to'>,\n",
    "    \"object\": <the object of the action, expressed in one word if possible such as 'beach', NOT 'the beach'>,\n",
    "}\n",
    "Use an empty string when these fields are not available.\n",
    "Do not return a belief if the subject and actions are not clearly identified.\n",
    "\n",
    "I want general answers, not specific to this image.\n",
    "For instance, for this image, I would expect something like:\n",
    "{\n",
    "    \"condition\": \"sunny day\",\n",
    "    \"objective\": \"enjoy the countryside\",\n",
    "    \"subject\": \"people\",\n",
    "    \"action\": \"drive\",\n",
    "    \"object\": \"convertible\",\n",
    "}\n",
    "not the following\n",
    "{\n",
    "    \"condition\": \"in a sunny day\",\n",
    "    \"objective\": \"because he enjoys the countryside\",\n",
    "    \"subject\": \"the person in the driver seat\",\n",
    "    \"action\": \"drive\",\n",
    "    \"object\": \"the convertible\",\n",
    "}\n",
    "For \"subject\", \"action\", \"object\", be as generic and short as possible.\n",
    "Do this for all the beliefs you can extract from the image.\n",
    "\"\"\"\n",
    "ans1 = prompt_image(prompt2, image_path)\n",
    "print(ans1.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this car is a convertible because the roof can be removed\n",
      "the person in the passenger seat is waving a scarf for enjoyment in a moving car\n",
      "the person in the driver's seat is driving this car to navigate the vehicle on the road\n",
      "the people are wearing sunglasses for eye protection in sunny weather\n",
      "the landscape includes trees indicative of a rural or semi-rural area\n",
      "the car is on a road suggesting the action of traveling\n",
      "the sky is clear suggesting it is fair weather\n",
      "the structure in the background is a building possibly for residential or historical significance in a rural setting\n"
     ]
    }
   ],
   "source": [
    "prompt3 = \"\"\" \n",
    "list all possible beliefs we can extract from this image, and express them in this format\n",
    "<this thing or person> <action> <another thing><for this reason or purpose><in these conditions ><optional>... \n",
    "Please keep the propositions with the <action> to leave <another thing> as an object/person name\n",
    "\n",
    "You can also use another format, when there is no action, using the verb 'to be', such as:\n",
    "<this car><is><a convertible><because the roof can be removed>\n",
    "\n",
    "To generate all possible beliefs, in an exhaustive way, look at all the objects, persons, background etc and try to find a relation between them following the formats above.\n",
    "Do not output anything before the first '<' and after the last '>'.\n",
    "Do not output the '<' or '>' characters.\n",
    "\"\"\"\n",
    "ans3 = prompt_image(prompt3, image_path)\n",
    "print(ans3.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "objects_and_people = [\n",
      "    \"sky\",\n",
      "    \"scarf\",\n",
      "    \"woman\",\n",
      "    \"sunglasses\",\n",
      "    \"car\",\n",
      "    \"trees\",\n",
      "    \"man\",\n",
      "    \"building\",\n",
      "    \"landscape\"\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt4 = \"\"\" \n",
    "Find all objects or people you can find this image and put them in a python list. \n",
    "Be exhaustive, do not miss any object or person, including in the background, the sky if there is one, through a window, behind people, even partially visible etc.\n",
    "Just list one object at a time, do not include what they are doing or where they are, just the object / person name only.\n",
    "For instance, \"women wearing hats\" is two objects \"women\" and \"hats\"\n",
    "\"\"\"\n",
    "ans4 = prompt_image(prompt4, image_path)\n",
    "print(ans4.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    \"sky\",',\n",
       " '    \"scarf\",',\n",
       " '    \"woman\",',\n",
       " '    \"sunglasses\",',\n",
       " '    \"car\",',\n",
       " '    \"trees\",',\n",
       " '    \"man\",',\n",
       " '    \"building\",',\n",
       " '    \"landscape\"']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans4.json()[\"choices\"][0][\"message\"][\"content\"].split(\"\\n\")[2:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'sky, scarf, woman, sunglasses, car, trees, man, building, landscape'\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objectPrompt=\"\"\"\n",
    "Find all the words between quotes in the following string and put them in a python string.\n",
    "Just output the string without any comment, start with ' and end with ], nothing else.\n",
    "Do not put quotes or double quotes around the words.\n",
    "\"\"\"\n",
    "requestMessage = objectPrompt + '\\n' + ans4.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "objects = await client.chat.completions.create(model=\"gpt-4\",  # previous models, even GPT3.5 didn't work that well\n",
    "                                        messages=[{\"role\": \"system\", \"content\": \"You are an expert in linguistics, semantic\"},\n",
    "                                                    {\"role\": \"user\", \"content\": requestMessage}\n",
    "                                                    ]\n",
    "                                        )\n",
    "objects_in_image = objects.choices[0].message.content[1:-1].split(\", \")\n",
    "objects_in_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sky',\n",
       " 'scarf',\n",
       " 'woman',\n",
       " 'sunglasses',\n",
       " 'car',\n",
       " 'trees',\n",
       " 'man',\n",
       " 'building',\n",
       " 'landscape']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects_in_image = objects.choices[0].message.content[1:-1].split(\", \")\n",
    "objects_in_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['belief1: this car is a convertible because the roof can be removed',\n",
       " 'belief2: the person in the passenger seat waves a scarf for joy in bright weather conditions',\n",
       " 'belief3: the driver operates the car for transportation under clear sky conditions',\n",
       " 'belief4: both individuals wear sunglasses for eye protection in sunny conditions',\n",
       " 'belief5: the vegetation is typical of a Mediterranean environment because of the species and climate conditions present',\n",
       " 'belief6: the car is on a dirt road for travel amidst a rural landscape ',\n",
       " 'belief7: the building in the background is older for architectural style in a countryside setting',\n",
       " 'belief8: the individuals are likely friends for engaging in a leisure activity together in a vehicle',\n",
       " 'belief9: the scarf is colorful for aesthetic purposes in the daylight',\n",
       " 'belief10: the car color is teal for its paint in daylight conditions']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beliefs = [f\"belief{i}: \"+ans.replace('<','').replace('>',' ') for i,ans in enumerate(ans3.json()[\"choices\"][0][\"message\"][\"content\"].split(\"\\n\"), 1)]\n",
    "beliefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token  30146  decodes to  Lab\n",
      "Token  30146  decodes to  Lab\n",
      "Token  374  decodes to   is\n",
      "Token  8056  decodes to   amazing\n",
      "Token  0  decodes to  !\n",
      "Number of tokens in corpus  4\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "token_list = enc.encode(\"LabLab is amazing!\")\n",
    "for token in token_list:\n",
    "  print (\"Token \", str(token), \" decodes to \", enc.decode([token]))\n",
    "\n",
    "print (\"Number of tokens in corpus \", len(enc.encode(\"YOUR_INPUT_CHOICE\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jpb2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def split_text (input_text):\n",
    "  split_texts = sent_tokenize(input_text)\n",
    "  return split_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(split_sents, max_token_len=2000):\n",
    "  current_token_len = 0\n",
    "  input_chunks = []\n",
    "  current_chunk = \"\"\n",
    "  for sents in split_sents:\n",
    "    sent_token_len = len(enc.encode(sents))\n",
    "    if (current_token_len + sent_token_len) > max_token_len:\n",
    "      input_chunks.append(current_chunk) # we reached the limit, add new chunk 'current chunk', then reset it\n",
    "      current_chunk = \"\"\n",
    "      current_token_len = 0\n",
    "    current_chunk = current_chunk + sents\n",
    "    current_token_len = current_token_len + sent_token_len\n",
    "  if current_chunk != \"\":\n",
    "    input_chunks.append(current_chunk)\n",
    "  return input_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sents = split_text(beliefs)\n",
    "input_chunks = create_chunks(split_sents, max_token_len=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructPrompt = \"\"\"\n",
    "You are an expert in linguistics, semantic and you are trying to format the beliefs passed to you into a format that can be stored in a knowledge graph.\n",
    "The beliefs start with the word 'belief1:', 'belief2:' etc and are separated by a new line.\n",
    "\n",
    "Rewrite every belief and express them as a python dictionary with the following format:\n",
    "{\n",
    "    \"condition\": <conditions observed from the picture such as a sunny day, ie the conditions leading to the rest of the beliefs, such as the objective and action>,\n",
    "    \"objective\": <the objective of the person or thing in the picture, after observing the conditions in the picture>,\n",
    "    \"subject\": <the person or thing in the picture doing the 'action' to meet the 'objective', just one word if possible>,\n",
    "    \"action\": <the action the person or thing in the picture is doing to meet the 'objective', expressed in one word with an optional preposition, such as 'drive to'>,\n",
    "    \"object\": <the object of the action, expressed in one word if possible such as 'beach', NOT 'the beach'>\n",
    "}\n",
    "Use an empty string when a field is not available.\n",
    "Do not return a belief if the subject and actions are not clearly identified.\n",
    "\n",
    "I want general answers, not specific to an image.\n",
    "For instance, I would expect something like:\n",
    "{\n",
    "    \"condition\": \"sunny day\",\n",
    "    \"objective\": \"enjoy the countryside\",\n",
    "    \"subject\": \"people\",\n",
    "    \"action\": \"drive\",\n",
    "    \"object\": \"convertible\"\n",
    "}\n",
    "not the following\n",
    "{\n",
    "    \"condition\": \"in a sunny day\",\n",
    "    \"objective\": \"because he enjoys the countryside\",\n",
    "    \"subject\": \"the person in the driver seat\",\n",
    "    \"action\": \"drive\",\n",
    "    \"object\": \"the convertible\"\n",
    "}\n",
    "For \"subject\", \"action\", \"object\", be as generic and short as possible.\n",
    "\n",
    "When the belief does not have an action, but instead use a verb such as 'to be' for instance, then put the verb in the <action anyway>, \n",
    "and use the field 'object' to describe what the object or person is or is made of or anything else that the verb describes.\n",
    "\n",
    "Use infinitive verbs, without the 'to', ie 'be' instead of 'is'.  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "requestMessages = [instructPrompt + '\\n' + belief for belief in beliefs]\n",
    "chatOutputs = []\n",
    "for request in requestMessages: \n",
    "  chatOutput = await client.chat.completions.create(model=\"gpt-4\",  # previous models, even GPT3.5 didn't work that well\n",
    "                                            messages=[{\"role\": \"system\", \"content\": \"You are an expert in linguistics, semantic\"},\n",
    "                                                      {\"role\": \"user\", \"content\": request}\n",
    "                                                      ]\n",
    "                                            )\n",
    "  chatOutputs.append(chatOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'condition': 'roof can be removed', 'objective': '', 'subject': 'car', 'action': 'be', 'object': 'convertible'}\n",
      "{'condition': 'bright weather conditions', 'objective': 'express joy', 'subject': 'person', 'action': 'wave', 'object': 'scarf'}\n",
      "{'condition': 'clear sky', 'objective': 'transportation', 'subject': 'driver', 'action': 'operate', 'object': 'car'}\n",
      "{'condition': 'sunny conditions', 'objective': 'eye protection', 'subject': 'individuals', 'action': 'wear', 'object': 'sunglasses'}\n",
      "{'condition': 'Mediterranean environment', 'objective': '', 'subject': 'vegetation', 'action': 'be', 'object': 'typical'}\n",
      "{'condition': 'rural landscape', 'objective': 'travel', 'subject': 'car', 'action': 'be', 'object': 'on a dirt road'}\n",
      "{'condition': 'countryside setting', 'objective': '', 'subject': 'building', 'action': 'be', 'object': 'older'}\n",
      "{'condition': 'engaging in leisure activity together', 'objective': '', 'subject': 'individuals', 'action': 'be', 'object': 'friends'}\n",
      "{'condition': 'daylight', 'objective': 'aesthetic purposes', 'subject': 'scarf', 'action': 'be', 'object': 'colorful'}\n",
      "{'condition': 'daylight conditions', 'objective': '', 'subject': 'car', 'action': 'be', 'object': 'teal'}\n"
     ]
    }
   ],
   "source": [
    "formatted_beliefs = {}\n",
    "for belief in chatOutputs:\n",
    "  b = json.loads(belief.choices[0].message.content)\n",
    "  formatted_beliefs[b[\"subject\"]] = b\n",
    "  print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Connected\n"
     ]
    }
   ],
   "source": [
    "# I setup a free Neo4J instance on Aura, and I'm using the Python driver to connect to it\n",
    "import json, configparser\n",
    "from py2neo import Relationship\n",
    "\n",
    "Neo4j_config = configparser.ConfigParser()\n",
    "Neo4j_config.read('neo4j_config.ini')\n",
    "Neo4j_config['DEFAULT'][\"pw\"] = environ['NEO4J_AURA_PW']\n",
    "\n",
    "g = Neo4jGraph(showstatus=True, **Neo4j_config['DEFAULT'])\n",
    "\n",
    "# in case of error, check your config.ini file\n",
    "\n",
    "# Instructions to see the graph in Neo4J Browser\n",
    "# go to \n",
    "# https://workspace-preview.neo4j.io/workspace/query?ntid=auth0%7C631bb4216f68981ab949290b\n",
    "# run the cypher query: \n",
    "# MATCH (n) RETURN n     to see all nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All nodes have been deleted!\n",
      "Graph has been cleaned\n"
     ]
    }
   ],
   "source": [
    "g.deleteAllNodes(DEBUG=True)\n",
    "assert g.nodesNb == 0\n",
    "print(\"Graph has been cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All nodes have been deleted!\n",
      "Graph has been cleaned\n",
      "car\n",
      "be\n",
      "convertible\n",
      "\n",
      "the roof can be removed\n",
      "New node ('car',) created with ID = 13\n",
      "New node ('convertible',) created with ID = 14\n",
      "Relationship 'be' created between nodes 13 and node 14\n",
      "==================================================\n",
      "person\n",
      "wave\n",
      "scarf\n",
      "express joy\n",
      "bright weather conditions\n",
      "New node ('person',) created with ID = 0\n",
      "New node ('scarf',) created with ID = 1\n",
      "Relationship 'wave' created between nodes 0 and node 1\n",
      "==================================================\n",
      "driver\n",
      "operate\n",
      "car\n",
      "transportation\n",
      "clear sky conditions\n",
      "New node ('driver',) created with ID = 2\n",
      "\u001b[91m\u001b[1mNode ('car',) already exists with ID = 13\u001b[0m\n",
      "Relationship 'operate' created between nodes 2 and node 13\n",
      "==================================================\n",
      "individuals\n",
      "wear\n",
      "sunglasses\n",
      "eye protection\n",
      "sunny conditions\n",
      "New node ('individuals',) created with ID = 3\n",
      "New node ('sunglasses',) created with ID = 4\n",
      "Relationship 'wear' created between nodes 3 and node 4\n",
      "==================================================\n",
      "vegetation\n",
      "be\n",
      "typical\n",
      "\n",
      "Mediterranean environment\n",
      "New node ('vegetation',) created with ID = 5\n",
      "New node ('typical',) created with ID = 6\n",
      "Relationship 'be' created between nodes 5 and node 6\n",
      "==================================================\n",
      "car\n",
      "be\n",
      "on a dirt road\n",
      "travel\n",
      "rural landscape\n",
      "\u001b[91m\u001b[1mNode ('car',) already exists with ID = 13\u001b[0m\n",
      "New node ('on a dirt road',) created with ID = 7\n",
      "Relationship 'be' created between nodes 13 and node 7\n",
      "==================================================\n",
      "building\n",
      "be\n",
      "older for architectural style\n",
      "\n",
      "countryside setting\n",
      "New node ('building',) created with ID = 8\n",
      "New node ('older for architectural style',) created with ID = 9\n",
      "Relationship 'be' created between nodes 8 and node 9\n",
      "==================================================\n",
      "individuals\n",
      "be\n",
      "friends\n",
      "have fun\n",
      "engaging in a leisure activity together in a vehicle\n",
      "\u001b[91m\u001b[1mNode ('individuals',) already exists with ID = 3\u001b[0m\n",
      "New node ('friends',) created with ID = 10\n",
      "Relationship 'be' created between nodes 3 and node 10\n",
      "==================================================\n",
      "scarf\n",
      "be\n",
      "colorful\n",
      "aesthetic purposes\n",
      "daylight\n",
      "\u001b[91m\u001b[1mNode ('scarf',) already exists with ID = 1\u001b[0m\n",
      "New node ('colorful',) created with ID = 11\n",
      "Relationship 'be' created between nodes 1 and node 11\n",
      "==================================================\n",
      "car\n",
      "be\n",
      "teal\n",
      "\n",
      "daylight\n",
      "\u001b[91m\u001b[1mNode ('car',) already exists with ID = 13\u001b[0m\n",
      "New node ('teal',) created with ID = 12\n",
      "Relationship 'be' created between nodes 13 and node 12\n",
      "==================================================\n",
      "Nb of nodes = 15\n"
     ]
    }
   ],
   "source": [
    "g.deleteAllNodes(DEBUG=True)\n",
    "assert g.nodesNb == 0\n",
    "print(\"Graph has been cleaned\")\n",
    "\n",
    "for b in chatOutputs:\n",
    "  \n",
    "  belief = json.loads(b.choices[0].message.content)\n",
    "  \n",
    "  name=belief.pop(\"subject\")\n",
    "  object = belief.pop(\"object\")\n",
    "  relation = belief.pop(\"action\")\n",
    "  print('\\n'.join([name, relation, object, belief['objective'], belief['condition']]))\n",
    "  \n",
    "  try:\n",
    "    subject1  = createNode(g, name,\n",
    "                          user_id='JPB',\n",
    "                          display_name=name,\n",
    "                          labels_constraints=name,  # we don't create the same object/subject twice\n",
    "                          # properties_constraints=('user_id',), \n",
    "                          creation_timestamp = Now(),\n",
    "                          DEBUG=True)  # we can have the same subject appear several times\n",
    "    \n",
    "  except:\n",
    "    print(f\"node not created for {name}\")\n",
    "  \n",
    "  try:\n",
    "    object1  = createNode(g, object,\n",
    "                          user_id='JPB',\n",
    "                          display_name=object,\n",
    "                          labels_constraints=object,  \n",
    "                          properties_constraints=('user_id',), \n",
    "                          creation_timestamp = Now(),\n",
    "                          DEBUG=True,\n",
    "                          **belief,\n",
    "                          )\n",
    "  except:\n",
    "    print(f\"node not created for {object}\")\n",
    "    \n",
    "  try:\n",
    "    relat1 = createRelation(g, subject1, \n",
    "                            object1, \n",
    "                            relation,\n",
    "                            DEBUG=True,\n",
    "                            #synonyms='has imagery',  # for demo, relations can have properties as well\n",
    "                            allow_duplicates=True,   # Neo4J allows several identical relations between 2 nodes, but we don't want that\n",
    "                            counting=True)\n",
    "  except:\n",
    "    print(f\"relation not created for 'to {relation}'\")\n",
    "  \n",
    "  print(\"=\"*50)\n",
    "    \n",
    "print(\"Nb of nodes =\", g.nodesNb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "var link = document.createElement(\"link\");\n\tlink.rel = \"stylesheet\";\n\tlink.type = \"text/css\";\n\tlink.href = \"https://cdnjs.cloudflare.com/ajax/libs/vis/4.8.2/vis.css\";\n\tdocument.head.appendChild(link);\nrequire.config({     paths: {         vis: '//cdnjs.cloudflare.com/ajax/libs/vis/4.8.2/vis.min'     } }); require(['vis'], function(vis) {  window.vis = vis; }); ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import neo4jupyter\n",
    "neo4jupyter.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph represents the online Neo4J graph database.\n",
    "It's dynamic, you can move the nodes around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"62cf3b51-8c4c-42ae-9528-b86dc2887415\" style=\"height: 400px;\"></div>\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "    var nodes = [{\"id\": 13, \"label\": \"\", \"group\": \"car\", \"title\": \"Node('car', creation_timestamp='11/19/2023, 23:28:15', display_name='car', operate__counter=1, user_id='JPB')\"}, {\"id\": 12, \"label\": \"\", \"group\": \"teal\", \"title\": \"Node('teal', be__counter=1, condition='daylight', creation_timestamp='11/19/2023, 23:28:25', display_name='teal', objective='', user_id='JPB')\"}, {\"id\": 7, \"label\": \"\", \"group\": \"on a dirt road\", \"title\": \"Node('on a dirt road', be__counter=1, condition='rural landscape', creation_timestamp='11/19/2023, 23:28:21', display_name='on a dirt road', objective='travel', user_id='JPB')\"}, {\"id\": 14, \"label\": \"\", \"group\": \"convertible\", \"title\": \"Node('convertible', be__counter=1, condition='the roof can be removed', creation_timestamp='11/19/2023, 23:28:15', display_name='convertible', objective='', user_id='JPB')\"}, {\"id\": 3, \"label\": \"\", \"group\": \"individuals\", \"title\": \"Node('individuals', creation_timestamp='11/19/2023, 23:28:18', display_name='individuals', user_id='JPB')\"}, {\"id\": 10, \"label\": \"\", \"group\": \"friends\", \"title\": \"Node('friends', be__counter=1, condition='engaging in a leisure activity together in a vehicle', creation_timestamp='11/19/2023, 23:28:23', display_name='friends', objective='have fun', user_id='JPB')\"}, {\"id\": 4, \"label\": \"\", \"group\": \"sunglasses\", \"title\": \"Node('sunglasses', condition='sunny conditions', creation_timestamp='11/19/2023, 23:28:19', display_name='sunglasses', objective='eye protection', user_id='JPB', wear__counter=1)\"}, {\"id\": 8, \"label\": \"\", \"group\": \"building\", \"title\": \"Node('building', creation_timestamp='11/19/2023, 23:28:22', display_name='building', user_id='JPB')\"}, {\"id\": 9, \"label\": \"\", \"group\": \"older for architectural style\", \"title\": \"Node('older for architectural style', be__counter=1, condition='countryside setting', creation_timestamp='11/19/2023, 23:28:22', display_name='older for architectural style', objective='', user_id='JPB')\"}, {\"id\": 2, \"label\": \"\", \"group\": \"driver\", \"title\": \"Node('driver', creation_timestamp='11/19/2023, 23:28:17', display_name='driver', user_id='JPB')\"}, {\"id\": 1, \"label\": \"\", \"group\": \"scarf\", \"title\": \"Node('scarf', condition='bright weather conditions', creation_timestamp='11/19/2023, 23:28:17', display_name='scarf', objective='express joy', user_id='JPB', wave__counter=1)\"}, {\"id\": 11, \"label\": \"\", \"group\": \"colorful\", \"title\": \"Node('colorful', be__counter=1, condition='daylight', creation_timestamp='11/19/2023, 23:28:24', display_name='colorful', objective='aesthetic purposes', user_id='JPB')\"}, {\"id\": 6, \"label\": \"\", \"group\": \"typical\", \"title\": \"Node('typical', be__counter=1, condition='Mediterranean environment', creation_timestamp='11/19/2023, 23:28:20', display_name='typical', objective='', user_id='JPB')\"}, {\"id\": 0, \"label\": \"\", \"group\": \"person\", \"title\": \"Node('person', creation_timestamp='11/19/2023, 23:28:16', display_name='person', user_id='JPB')\"}, {\"id\": 5, \"label\": \"\", \"group\": \"vegetation\", \"title\": \"Node('vegetation', creation_timestamp='11/19/2023, 23:28:20', display_name='vegetation', user_id='JPB')\"}];\n",
       "    var edges = [{\"from\": 13, \"to\": 12, \"label\": \"be\"}, {\"from\": 13, \"to\": 7, \"label\": \"be\"}, {\"from\": 13, \"to\": 14, \"label\": \"be\"}, {\"from\": 3, \"to\": 10, \"label\": \"be\"}, {\"from\": 3, \"to\": 4, \"label\": \"wear\"}, {\"from\": 8, \"to\": 9, \"label\": \"be\"}, {\"from\": 2, \"to\": 13, \"label\": \"operate\"}, {\"from\": 1, \"to\": 11, \"label\": \"be\"}, {\"from\": 0, \"to\": 1, \"label\": \"wave\"}, {\"from\": 5, \"to\": 6, \"label\": \"be\"}];\n",
       "\n",
       "    var container = document.getElementById(\"62cf3b51-8c4c-42ae-9528-b86dc2887415\");\n",
       "\n",
       "    var data = {\n",
       "        nodes: nodes,\n",
       "        edges: edges\n",
       "    };\n",
       "\n",
       "    var options = {\n",
       "    nodes: {\n",
       "        shape: 'dot',\n",
       "        size: 25,\n",
       "        font: {\n",
       "            size: 14\n",
       "        }\n",
       "    },\n",
       "    edges: {\n",
       "        font: {\n",
       "            size: 14,\n",
       "            align: 'middle'\n",
       "        },\n",
       "        color: 'gray',\n",
       "        arrows: {\n",
       "            to: {\n",
       "                enabled: true,\n",
       "                scaleFactor: 0.5\n",
       "            }\n",
       "        },\n",
       "        smooth: {\n",
       "            enabled: false\n",
       "        }\n",
       "    },\n",
       "    physics: {\n",
       "        enabled: true\n",
       "        }\n",
       "    };\n",
       "\n",
       "    var network = new vis.Network(container, data, options);\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see https://github.com/merqurio/neo4jupyter/blob/master/README.md\n",
    "neo4jupyter.draw(g.graph, {\"user_id\": \"JPB\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
